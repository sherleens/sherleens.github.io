<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noindex">
        <title>
Home - Dongyu She</title>

        <!-- Bootstrap -->
        <link href="./Home - DongyuShe_files/bootstrap.css" rel="stylesheet">
        <link href="./Home - DongyuShe_files/docs.css" rel="stylesheet">
        <link href="./Home - DongyuShe_files/custom.css" rel="stylesheet">
        <link href="./Home - DongyuShe_files/font-awesome/css/font-awesome.min.css" rel="stylesheet">
    </head>

<body>
    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="./Home - DongyuShe_files/jquery-1.11.2.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="./Home - DongyuShe_files/bootstrap.min.js"></script>
    <div class="container">
        

<div class="row">
    <div class="col-sm-3" style="padding-top: 50px;">
        <img src="./Home - DongyuShe_files/images/myPh.jpg" style="width: 100%; max-width: 180px"><br>
        <h3>Dongyu She</h3>
        <p>
            Ph.D. student
        </p>
        <p>
            East Main Building 9-211<br>
            Tsinghua Univeristy<br>
            Beijing, 100084<br>
            China
        </p>
        <p>
        	<i class="fa fa-envelope-o" aria-hidden="true"></i><font size="1"> shedy19[at]mails.tsinghua.edu.cn</font><br>
        	
        </p>
    </div>

    <div class="col-sm-8" style="padding-top: 20px;">        
        
        <h2>Welcome</h2>
        <div class="well"> 
        <p class="text-justify">
            I am a PhD student at the <a href="http://cg.cs.tsinghua.edu.cn/" target="_blank">Graphics and Geometric Computing Group</a> (CSCG), <a href="http://www.tsinghua.edu.cn" target="_blank">Tsinghua University</a>, China. I am supervised by Prof. <a href="https://cg.cs.tsinghua.edu.cn/people/~kun/" target="_blank">Kun Xu</a>. I received my master and bachelor degree from <a href="https://cv.nankai.edu.cn" target="_blank">Computer Vision Lab</a>, Department of Computer Science, <a href="http://http://www.nankai.edu.cn/" target="_blank">Nankai University</a> in 2016 and in 2019, respectively. My research interests are primarily on computer vision and deep learning. 
        </p>
        </div>
        
        
<div class="row">
     <div class="col-sm-12">
          <h2>Latest News</h2>
          <div class="list-group">
          	<a href="#pub:She2019b" class="list-group-item">
              2019.08 <font color=red>[Paper]</font> One paper is accepted to Transactions on Multimedia (TMM 2019)
            </a>
            <a href="http://www.icme2019.org/conf_workshops" class="list-group-item">
              2019.07 <font color=blue>[Acitivity]</font>  I give an invited talk on Visual Emotion Analysis: Theories and Applications in ICME2019 Workshop at Shanghai, China
            </a>
            <a href="#pub:She2019a" class="list-group-item">
              2019.04 <font color=red>[Paper]</font><font color=green>[Data]</font> One paper is accepted to Transactions on Multimedia Computing Communications and Applications (TOMM 2019) & ComicDataset available
            </a>
            <a href="http://valse2019.org" class="list-group-item">
              2019.04 <font color=blue>[Acitivity]</font>  I attend the VALSE 2019 at Hefei, China
            </a>
            <!-- a href="" class="list-group-item">
              2019.01 <font color=green>[Data]</font>  Code available for TMM 2018 & IJCAI 2017
            </a-->
            <a href="" class="list-group-item">
              2018.11 <font color=purple>[Award]</font> I receive the National Scholarship for graduate students
            </a>
          </div>
      </div>

</div>


<h2>Academic Services</h2>

        <ul>
          <li>Program Committee Member, AAAI2020</li>
          <li>Program Committee Member, MMM2020</li>
          <li>Reviewer, TIP, NPL, TOMM</li>
          <li>Reviewer, CVPR2019, ICCV2019, PRCV2019</li>
        </ul>

<br>

<h2>Selected Publications[<a href="https://scholar.google.com/citations?hl=zh-CN&user=jK8qU70AAAAJ&amp;hl=en" target="_blank">Google Scholor</a>]</h2>
        
<div style="padding-top: 6px; padding-bottom: 6px;"> <h3>Journal Articles</h3> </div>

<div class="row" style="padding-bottom: 10px;"><a class="publication" name="pub:She2019b"></a>
    <div class="col-sm-3">
        <img src="./Home - DongyuShe_files/images/tmm19.jpg" width="100%" style="min-width:120px; max-width: 240px; display:block; margin:5; auto" alt="She_2019_TMM">
    </div>
    <div class="col-sm-9">
        <h4>WSCNet: Weakly Supervised Coupled Networks for Visual Sentiment Classification and Detection</h4>
        <p>
            Dongyu She, Jufeng Yang, Ming-Ming Cheng, Yu-Kun Lai, Paul L. Rosin and Liang Wang<br>
            IEEE Transcation on Multimedia
            <!-- ADD SHORT NAME WITH LINK TO URL -->
            
                (<a href="https://signalprocessingsociety.org/publications-resources/ieee-transactions-multimedia" target="_blank">TMM</a>),
                        
            <!-- ADD LOCATION -->
            
                
            
            <!-- ADD MONTH, YEAR -->

            2019[accepted].
            
            <!-- ADD INFO -->
            
        </p>
        <p>
        </p><div class="btn-toolbar" role="toolbar" aria-label="...">

        	<!-- ADD PDF BUTTON -->

            <div class="btn-group" role="group"><a type="button" class="btn btn-primary btn-sm" href="">PDF</a></div>

            <!-- ADD BIB BUTTON -->

            <div class="btn-group" role="group"><button type="button" class="btn btn-info btn-sm" data-toggle="collapse" data-target="#bibtex_She_2019_TMM">BibTeX</button></div>

            <!-- ADD PROJECT PAGE BUTTON -->
            
             <div class="btn-group" role="group"><a type="button" class="btn btn-success btn-sm" href="https://github.com/sherleens/WSCNet" target="_blank">Project</a></div>
            
            <!-- ADD ABSTRACT BUTTON-->
            
            <div class="btn-group" role="group"><button type="button" class="btn btn-default btn-sm" data-toggle="collapse" data-target="#abstract_She_2019_TMM">Abstract</button></div>
                
        <p></p>

        <div id="bibtex_She_2019_TMM" class="collapse" style="font-family:monospace;"><br>@article{She_2019_TMM,<br> author = {She, Dongyu and Yang, Jufeng and Cheng, Ming-Ming and Lai, Yu-Kun and Rosin, Paul L. and Wang, Liang},<br> title = {WSCNet: Weakly Supervised Coupled Networks for Visual Sentiment Classification and Detection},<br> journal={IEEE Transactions on Multimedia},<br> year = {2019}<br>}</div>

        <div id="abstract_She_2019_TMM" class="collapse" style="text-align:justify;color:#606060;"><br>Automatic assessment of sentiment from visual content has gained considerable attention with the increasing tendency of expressing opinions online. In this paper, we solve the problem of visual sentiment analysis, which is challenging due to the high-level abstraction in the recognition process. Existing methods based on convolutional neural networks learn sentiment representations from the holistic image, despite the fact that different image regions can have different influence on the evoked sentiment. In this paper, we introduce a weakly supervised coupled convolutional network (WSCNet). Our method is dedicated to automatically selecting relevant soft proposals from weak annotations (\eg global image labels), thereby significantly reducing the annotation burden, and encompasses the following contributions. First, WSCNet detects a sentiment-specific soft map by training a fully convolutional network with the cross spatial pooling strategy in the detection branch. Second, both the holistic and localized information are utilized by coupling the sentiment map with deep features for robust representation in the classification branch. We integrate the sentiment detection and classification branches into a unified deep framework, and optimize the network in an end-to-end way. Through this joint learning strategy, weakly supervised sentiment classification and detection benefit each other. Extensive experiments demonstrate that the proposed WSCNet outperforms the state-of-the-art results on seven benchmark datasets.</div>
        <br>
        </div>
    </div>
</div>

<div class="row" style="padding-bottom: 10px;"><a class="publication" name="pub:She2019a"></a>
    <div class="col-sm-3">
        <img src="./Home - DongyuShe_files/images/tomm19.jpg" width="100%" style="min-width:120px; max-width: 240px; display:block; margin:5; auto" alt="She_2019_TOMM">
    </div>
    <div class="col-sm-9">
        <h4>Learning Discriminative Sentiment Representation from Strongly- and Weakly-Supervised CNNs</h4>
        <p>
            Dongyu She, Jufeng Yang and Ming Sun<br>
            ACM Transactions on Multimedia Computing Communications and Applications
            <!-- ADD SHORT NAME WITH LINK TO URL -->
            
                (<a href="https://tomm.acm.org/">TOMM</a>),
                        
            <!-- ADD LOCATION -->
            
                
            
            <!-- ADD MONTH, YEAR -->

            2019[accepted].
            
            <!-- ADD INFO -->
            
        </p>
        <p>
        </p><div class="btn-toolbar" role="toolbar" aria-label="...">

            <!-- ADD PDF BUTTON -->

            <div class="btn-group" role="group"><a type="button" class="btn btn-primary btn-sm" href="">PDF</a></div>

            <!-- ADD BIB BUTTON -->

            <div class="btn-group" role="group"><button type="button" class="btn btn-info btn-sm" data-toggle="collapse" data-target="#bibtex_She_2019_TOMM">BibTeX</button></div>

            <!-- ADD PROJECT PAGE BUTTON -->
            
             <div class="btn-group" role="group"><a type="button" class="btn btn-success btn-sm" href="https://github.com/sherleens/Comics" target="_blank">Dataset</a></div>
            
            <!-- ADD ABSTRACT BUTTON-->
            
            <div class="btn-group" role="group"><button type="button" class="btn btn-default btn-sm" data-toggle="collapse" data-target="#abstract_She_2019_TOMM">Abstract</button></div>
                
        <p></p>

        <div id="bibtex_She_2019_TOMM" class="collapse" style="font-family:monospace;"><br>@article{She_2019_TOMM,<br> author = {She, Dongyu and Yang, Jufeng and Sun, Ming},<br> title = {Learning Discriminative Sentiment Representation from Strongly- and Weakly-Supervised CNNs},<br> journal={ACM Transactions on Multimedia Computing Communications and Applications},<br> year = {2019}<br>}</div>

        <div id="abstract_She_2019_TOMM" class="collapse" style="text-align:justify;color:#606060;"><br>Visual sentiment analysis is attracting increasing attention with the rapidly growing amount of images uploaded to social network. Learning rich visual representations often requires training deep convolutional neural networks on massive manually labeled data, which is expensive or scarce especially for a subjective task like visual sentiment analysis. Meanwhile, a large quantity of social images is quite available yet noisy by querying social network using the sentiment categories as keywords, where a various type of images related to the specific sentiment can be easily collected. In this paper, we propose a multiple kernel network (MKN) for visual sentiment recognition, which learns representation from strongly- and weakly- supervised CNNs. Specifically, the weakly-supervised deep model is trained using the large-scale data from social images, while the strongly- supervised deep model is fine-tuned on the affecitve datasets with manual annotation. We employ the multiple kernel scheme on the multiple layers of CNNs, which can automatically select the discriminative representation by learning a linear combination from a set of pre-defined kernels. In addition, we introduce a large-scale dataset collected from popular comics of various countries, e.g., America, Japan, China and France, which consists of 11,821 images with various artistic styles. Experimental results show that MKN achieves consistent improvements over the state-of-the-art methods on the public affective datasets as well as the newly established Comics dataset.</div>
        <br>
        </div>
    </div>
</div>


<div class="row" style="padding-bottom: 10px;"><a class="publication" name="pub:She2018b"></a>
    <div class="col-sm-3">
        <img src="./Home - DongyuShe_files/images/tmm18.jpg" width="100%" style="min-width:120px; max-width: 240px; display:block; margin:5; auto" alt="She_2018_TMM">
    </div>
    <div class="col-sm-9">
        <h4>Visual Sentiment Prediction based on Automatic Discovery of Affective Regions</h4>
        <p>
            Jufeng Yang, Dongyu She, Ming Sun, Ming-Ming Cheng, Paul L. Rosin, Liang Wang<br>
            IEEE Transcation on Multimedia
            <!-- ADD SHORT NAME WITH LINK TO URL -->
            
                (<a href="https://signalprocessingsociety.org/publications-resources/ieee-transactions-multimedia" target="_blank">TMM</a>),
                        
            <!-- ADD LOCATION -->
            
                
            
            <!-- ADD MONTH, YEAR -->

            2018.
            
            <!-- ADD INFO -->
            
        </p>
        <p>
        </p><div class="btn-toolbar" role="toolbar" aria-label="...">

            <!-- ADD PDF BUTTON -->

            <div class="btn-group" role="group"><a type="button" class="btn btn-primary btn-sm" href="https://core.ac.uk/download/pdf/146501157.pdf">PDF</a></div>

            <!-- ADD BIB BUTTON -->

            <div class="btn-group" role="group"><button type="button" class="btn btn-info btn-sm" data-toggle="collapse" data-target="#bibtex_She_2018_TMM">BibTeX</button></div>

            <!-- ADD PROJECT PAGE BUTTON -->
            
             <div class="btn-group" role="group"><a type="button" class="btn btn-success btn-sm" href="https://mmcheng.net/vsp/" target="_blank">Project</a></div>
            
            <!-- ADD ABSTRACT BUTTON-->
            
            <div class="btn-group" role="group"><button type="button" class="btn btn-default btn-sm" data-toggle="collapse" data-target="#abstract_She_2018_TMM">Abstract</button></div>

            <!-- ADD OTHER BUTTON-->
            
            <div class="btn-group" role="group"><a type="button" class="btn btn-primary btn-sm" href="./Home - DongyuShe_files/files/TMM-CN.pdf">CN ver.</a></div>
                
        <p></p>

        <div id="bibtex_She_2018_TMM" class="collapse" style="font-family:monospace;"><br>@article{Yang_2018_TMM,<br> author = {Yang, Jufeng and She, Dongyu and Ming, Sun and Cheng, Ming Ming and Rosin, Paul and Liang, Wang},<br> title = {Visual Sentiment Prediction based on Automatic Discovery of Affective Regions},<br> journal={IEEE Transactions on Multimedia},<br> volume={20},<br>number={9},<br>pages={2513--2525},<br>year={2018}<br>}</div>

        <div id="abstract_She_2018_TMM" class="collapse" style="text-align:justify;color:#606060;"><br>Automatic assessment of sentiment from visual content has gained considerable attention with the increasing tendency of expressing opinions via images and videos online. This paper investigates the problem of visual sentiment analysis, which involves a high-level abstraction in the recognition process. While most of the current methods focus on improving holistic representations, we aim to utilize the local information, which is inspired by the observation that both the whole image and local regions convey significant sentiment information. We propose a framework to leverage affective regions, where we first use an off-the-shelf objectness tool to generate the candidates, and employ a candidate selection method to remove redundant and noisy proposals. Then a convolutional neural network (CNN) is connected with each candidate to compute the sentiment scores, and the affective regions are automatically discovered, taking the objectness score as well as the sentiment score into consideration. Finally, the CNN outputs from local regions are aggregated with the whole images to produce the final predictions. Our framework only requires image-level labels, thereby significantly reducing the annotation burden otherwise required for training. This is especially important for sentiment analysis as sentiment can be abstract, and labeling affective regions is too subjective and labor-consuming. Extensive experiments show that the proposed algorithm outperforms the state-of-the-art approaches on eight popular benchmark datasets.</div>
        <br>
        </div>
    </div>
</div>



<div style="padding-top: 6px; padding-bottom: 6px;"> <h3>Conference Proceedings</h3> </div>
 
<div class="row" style="padding-bottom: 10px;"><a class="publication" name="pub:She2019b"></a>
    <div class="col-sm-3">
        <img src="./Home - DongyuShe_files/images/cvpr18.jpg" width="100%" style="min-width:120px; max-width: 240px; display:block; margin:5; auto" alt="Yang_2018_CVPR">
    </div>
    <div class="col-sm-9">
        <h4>Weakly Supervised Coupled Networks for Visual Sentiment Analysis</h4>
        <p>
            Jufeng Yang, Dongyu She, Yu-Kun Lai, Paul L. Rosin, Ming-Hsuan Yang<br>
            IEEE Conference on Computer Vision and Pattern Recognition
            <!-- ADD SHORT NAME WITH LINK TO URL -->
            
                (<a href="http://cvpr2018.thecvf.com/" target="_blank">CVPR</a>),
                        
            <!-- ADD LOCATION -->
            
                Salt Lake City, USA,
            
            <!-- ADD MONTH, YEAR -->

            June. 2018.
            
            <!-- ADD INFO -->
            
        </p>
        <p>
        </p><div class="btn-toolbar" role="toolbar" aria-label="...">

        	<!-- ADD PDF BUTTON -->

            <div class="btn-group" role="group"><a type="button" class="btn btn-primary btn-sm" href="http://openaccess.thecvf.com/content_cvpr_2018/html/2108.html">PDF</a></div>

            <!-- ADD BIB BUTTON -->

            <div class="btn-group" role="group"><button type="button" class="btn btn-info btn-sm" data-toggle="collapse" data-target="#bibtex_Yang_2018_CVPR">BibTeX</button></div>

            <!-- ADD PROJECT PAGE BUTTON -->
            
             <div class="btn-group" role="group"><a type="button" class="btn btn-success btn-sm" href="https://github.com/sherleens/WSCNet" target="_blank">Project</a></div>
            
            <!-- ADD ABSTRACT BUTTON-->
            
            <div class="btn-group" role="group"><button type="button" class="btn btn-default btn-sm" data-toggle="collapse" data-target="#abstract_Yang_2018_CVPR">Abstract</button></div>
                
        <p></p>

        <div id="bibtex_Yang_2018_CVPR" class="collapse" style="font-family:monospace;"><br>@InProceedings{Yang_2018_CVPR,<br> author = {Yang, Jufeng and She, Dongyu and Lai, Yu-Kun and Rosin, Paul L. and Yang, Ming-Hsuan},<br> title = {Weakly Supervised Coupled Networks for Visual Sentiment Analysis},<br> booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},<br> month = {June},year = {2018}<br>}</div>
        <div id="abstract_Yang_2018_CVPR" class="collapse" style="text-align:justify;color:#606060;"><br>Automatic assessment of sentiment from visual content has gained considerable attention with the increasing tendency of expressing opinions on-line. In this paper, we solve the problem of visual sentiment analysis using the high-level abstraction in the recognition process. Existing methods based on convolutional neural networks learn sentiment representations from the holistic image appearance. However, different image regions can have a different influence on the intended expression. This paper presents a weakly supervised coupled convolutional network with two branches to leverage the localized information. The first branch detects a sentiment specific soft map by training a fully convolutional network with the cross spatial pooling strategy, which only requires image-level labels, thereby significantly reducing the annotation burden. The second branch utilizes both the holistic and localized information by coupling the sentiment map with deep features for robust classification. We integrate the sentiment detection and classification branches into a unified deep framework and optimize the network in an end-to-end manner. Extensive experiments on six benchmark datasets demonstrate that the proposed method performs favorably against the state-ofthe-art methods for visual sentiment analysis.</div>
        <br>
        </div>
    </div>
</div>


<div class="row" style="padding-bottom: 10px;"><a class="publication" name="pub:She2018a"></a>
    <div class="col-sm-3">
        <img src="./Home - DongyuShe_files/images/aaai18.jpg" width="100%" style="min-width:120px; max-width: 240px; display:block; margin:5; auto" alt="Yang_2018_AAAI">
    </div>
    <div class="col-sm-9">
        <h4>Retrieving and Classifying Affective Images via Deep Metric Learning</h4>
        <p>
            Jufeng Yang, Dongyu She, Yu-Kun Lai, Ming-Hsuan Yang<br>
            AAAI Conference on Artificial Intelligence
            <!-- ADD SHORT NAME WITH LINK TO URL -->
            
                (<a href="https://aaai.org/Conferences/AAAI-18/" target="_blank">AAAI</a>),
                        
            <!-- ADD LOCATION -->
            
                New Orleans, Louisiana, USA,
            
            <!-- ADD MONTH, YEAR -->

            Feb. 2018.
            
            <!-- ADD INFO -->
            
        </p>
        <p>
        </p><div class="btn-toolbar" role="toolbar" aria-label="...">

            <!-- ADD PDF BUTTON -->

            <div class="btn-group" role="group"><a type="button" class="btn btn-primary btn-sm" href="http://orca-mwe.cf.ac.uk/106882/1/retrieving-classifying-affective-AAAI18.pdf">PDF</a></div>

            <!-- ADD BIB BUTTON -->

            <div class="btn-group" role="group"><button type="button" class="btn btn-info btn-sm" data-toggle="collapse" data-target="#bibtex_Yang_2018_AAAI">BibTeX</button></div>

            
            
            <!-- ADD ABSTRACT BUTTON-->
            
            <div class="btn-group" role="group"><button type="button" class="btn btn-default btn-sm" data-toggle="collapse" data-target="#abstract_Yang_2018_AAAI">Abstract</button></div>


             <!-- ADD OTHER BUTTON-->
            
            <div class="btn-group" role="group"><a type="button" class="btn btn-primary btn-sm" href="./Home - DongyuShe_files/files/AAAI18_sentiment.pptx">Slides.</a></div>
                
        <p></p>

        <div id="bibtex_Yang_2018_AAAI" class="collapse" style="font-family:monospace;"><br>@InProceedings{Yang_2018_AAAI,<br> author = {Yang, Jufeng and She, Dongyu and Lai, Yu-Kun and Yang, Ming-Hsuan},<br> title = {Retrieving and Classifying Affective Images via Deep Metric Learning},<br> booktitle = {AAAI Conference on Artificial Intelligence (AAAI)},<br> month = {Feb},year = {2018}<br>}</div>
        <div id="abstract_Yang_2018_AAAI" class="collapse" style="text-align:justify;color:#606060;"><br>Affective image understanding has been extensively studied in the last decade since more and more users express emotion via visual contents. While current algorithms based on convolutional neural networks aim to distinguish emotional categories in a discrete label space, the task is inherently ambiguous. This is mainly because emotional labels with the same polarity (i.e., positive or negative) are highly related, which is different from concrete object concepts such as cat, dog and bird. To the best of our knowledge, few methods focus on leveraging such characteristic of emotions for affective image understanding. In this work, we address the problem of understanding affective images via deep metric learning and propose a multi-task deep framework to optimize both retrieval and classification goals. We propose the sentiment constraints adapted from the triplet constraints, which are able to explore the hierarchical relation of emotion labels. We further exploit the sentiment vector as an effective representation to distinguish affective images utilizing the texture representation derived from convolutional layers. Extensive evaluations on four widely-used affective datasets, i.e., Flickr and Instagram, IAPSa, Art Photo, and Abstract Paintings, demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods on both affective image retrieval and classification tasks.</div>
        <br>
        </div>
    </div>
</div>

<div class="row" style="padding-bottom: 10px;"><a class="publication" name="pub:She2017a"></a>
    <div class="col-sm-3">
        <img src="./Home - DongyuShe_files/images/ijcai17.jpg" width="100%" style="min-width:120px; max-width: 240px; display:block; margin:5; auto" alt="Yang_2017_IJCAI">
    </div>
    <div class="col-sm-9">
        <h4>Joint Image Emotion Classification and Distribution Learning via Deep Convolutional Neural Network</h4>
        <p>
            Jufeng Yang, Dongyu She, Ming Sun<br>
            International Joint Conference on Artificial Intelligence
            <!-- ADD SHORT NAME WITH LINK TO URL -->
            
                (<a href="http://www.ijcai-17.org/" target="_blank">IJCAI</a>),
                        
            <!-- ADD LOCATION -->
            
                Melbourne, Australia,
            
            <!-- ADD MONTH, YEAR -->

            Aug. 2017.
            
            <!-- ADD INFO -->
            
        </p>
        <p>
        </p><div class="btn-toolbar" role="toolbar" aria-label="...">

            <!-- ADD PDF BUTTON -->

            <div class="btn-group" role="group"><a type="button" class="btn btn-primary btn-sm" href="https://www.ijcai.org/proceedings/2017/0456.pdf">PDF</a></div>

            <!-- ADD BIB BUTTON -->

            <div class="btn-group" role="group"><button type="button" class="btn btn-info btn-sm" data-toggle="collapse" data-target="#bibtex_Yang_2017_IJCAI">BibTeX</button></div>

            <!-- ADD PROJECT PAGE BUTTON -->
            
             <div class="btn-group" role="group"><a type="button" class="btn btn-success btn-sm" href="https://github.com/sherleens/EmotionDistributionLearning" target="_blank">Project</a></div>
            
            <!-- ADD ABSTRACT BUTTON-->
            
            <div class="btn-group" role="group"><button type="button" class="btn btn-default btn-sm" data-toggle="collapse" data-target="#abstract_Yang_2017_IJCAI">Abstract</button></div>


                
        <p></p>

        <div id="bibtex_Yang_2017_IJCAI" class="collapse" style="font-family:monospace;"><br>@InProceedings{Yang_2017_IJCAI,<br> author = {Yang, Jufeng and She, Dongyu and Sun, Ming},<br> title = {Joint Image Emotion Classification and Distribution Learning via Deep Convolutional Neural Network},<br> booktitle = {International Joint Conference on Artificial Intelligence (IJCAI)},<br> month = {Aug},year = {2017}<br>}</div>
        <div id="abstract_Yang_2017_IJCAI" class="collapse" style="text-align:justify;color:#606060;"><br>Visual sentiment analysis is attracting more and more attention with the increasing tendency to express emotions through visual contents. Recentalgorithms in Convolutional Neural Networks (CNNs) considerably advance the emotion classification, which aims to distinguish differences among emotional categories and assigns a singledominant label to each image. However, the task is inherently ambiguous since an image usuallyevokes multiple emotions and its annotation varies from person to person. In this work, we address the problem via label distribution learning and develop a multi-task deep framework by jointly optimizing classification and distribution prediction. While the proposed method prefers to the distribution datasets with annotations of different voters, the majority voting scheme is widely adopted as the groundtruth in this area, and few dataset has providedmultiple affective labels. Hence, we further exploit two weak forms of prior knowledge, which are expressedas similarity information between labels, togenerate emotional distribution for each category. The experiments conducted on both distribution datasets, i.e. Emotion6, Flickr LDL, Twitter LDL, and the largest single label dataset, i.e. Flickr and Instagram, demonstrate the proposed method outperforms the state-of-the-art approaches.</div>
        <br>
        </div>
    </div>
</div>



</body></html>
